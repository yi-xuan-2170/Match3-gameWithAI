# Match-3遊戲與強化學習代理（DQN & PPO）\n\n本專案實作一個三消（Match-3）遊戲環境，並使用強化學習方法訓練智能代理（agents）來玩遊戲。主要比較與實作的演算法為深度Q網路（DQN）與近端策略優化（PPO）。\n\n## 主要內容\n- 環境：可重複操作的三消遊戲環境（狀態、動作、獎勵設計）。\n- 演算法：DQN（value-based）與PPO（policy-gradient）。\n- 訓練與評估：提供訓練腳本與評估工具，能紀錄學習曲線與遊戲表現。\n\n## 為什麼選擇 DQN 和 PPO？\n- DQN：適合離散動作空間（例如三消中每個交換動作可視為離散動作），容易實作且直觀地估計每個動作的價值。\n- PPO：穩定且樣本效率較好，屬於策略梯度方法，能直接優化策略，對一些需要長期規劃的情境（例如連鎖反應的最大化）表現較佳。\n\n## 三消遊戲（Match-3）簡述\n- 玩法：玩家通過交換相鄰方塊以形成至少三個連成一線的相同顏色方塊，消除後獲得分數並產生下落/補充。\n- 狀態（State）：一般包含棋盤格子的顏色矩陣，若有需要可包含當前分數、剩餘步數等。\n- 動作（Action）：選擇一個位置並與其相鄰方塊交換（通常4個方向或只允許交換產生消除的操作）。\n- 獎勵（Reward）：可設計為即時得分（每次消除得到的分數）或加入連鎖獎勵、回合結束獎勵等。\n\n## 系統架構（建議）\n1. Environment (env/)：實作OpenAI Gym-like介面（reset(), step(action), render()）。\n2. Agents (agents/)：DQN、PPO的實作（模型、訓練迴圈、記憶回放等）。\n3. Training scripts (train/)：單次訓練、超參數設定與紀錄。\n4. Evaluation & Visualization (eval/)：評估策略、畫學習曲線、遊戲重放。\n\n## 使用說明（快速上手）\n1. 安裝依賴（建議使用虛擬環境）：\n```bash\npython -m venv venv\nsource venv/bin/activate  # Windows: venv\\Scripts\\activate\npip install -r requirements.txt\n```\n\n2. 訓練 DQN（範例）：\n```bash\npython train/train_dqn.py --env Match3Env --episodes 2000 --save-dir models/dqn\n```\n\n3. 訓練 PPO（範例）：\n```bash\npython train/train_ppo.py --env Match3Env --timesteps 200000 --save-dir models/ppo\n```\n\n4. 測試與渲染（使用已訓練模型）：\n```bash\npython eval/play.py --model models/ppo/best.pt --env Match3Env --render\n```\n\n## 超參數建議\n- DQN：學習率 lr=1e-4、epsilon 起始 1.0 -> 0.1 緩慢衰減、batch size=32、memory size=1e5、target 更新頻率 1000 steps。\n- PPO：學習率 lr=3e-4、clip 範圍 0.2、update epochs=10、batch size=64。\n\n> 注意：根據環境與狀態表徵（例如是否採用CNN處理棋盤圖像或直接用 embedding）可能需要調整網路架構與超參數。\n\n## 評估指標\n- 平均分數（Mean Score）\n- 成功率（例如達到某分數門檻的回合比率）\n- 平均連鎖數（衡量策略是否能創造連鎖反應）\n- 學習曲線（Reward vs Episodes / Timesteps）\n\n## 範例結果（可放入訓練圖、影片連結）\n- DQN：在簡化版規則下可學會基本消除策略，但對於長連鎖規劃能力有限。\n- PPO：通常收斂更穩定，對連鎖策略與長期規劃表現較好。\n\n## 未來改進方向\n- 使用自監督學習或行為克隆初始化策略以加速收斂。\n- 加入層次化強化學習來處理較高級的目標（如最大化連鎖）。\n- 將觀察空間以影像表示，用CNN或Transformer做特徵抽取。\n\n## 參考與延伸閱讀\n- Mnih et al., 2015, Human-level control through deep reinforcement learning (DQN)\n- Schulman et al., 2017, Proximal Policy Optimization Algorithms (PPO)\n- OpenAI Gym: https://gym.openai.com/\n\n---\n\n如果你希望我把這個 README 寫進你的 repository，我可以直接幫你更新 main 分支的 README.md。若要我更新，請告訴我是否要使用上面的內容或需做任何語句/格式調整.